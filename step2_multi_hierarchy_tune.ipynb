{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Modelling\n","\n","### Hierarchical Models\n","We have tried to modularize majority of the model training into functions that can be reused for many different types of models and for different hierarchy levels. Since the data is so vast it is harder for models to converge especially if they have items that show very different behavior, so we aim to create models similiar timeseries together. We achieve this by creating subsets of data using on a single particular store/category/department while modeling. In this scenario we have N different models for the N different levels in the categorical column. In some cases we have also done this for nested hierarchies such as creating models per store per category."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1551,"status":"ok","timestamp":1668128392531,"user":{"displayName":"Divyansh Tripathi","userId":"01798682203976314315"},"user_tz":360},"id":"FAN1z-CMwXrc","outputId":"a0b6f559-4396-4c76-d54b-4547f469738d"},"outputs":[],"source":["import warnings\n","import pickle\n","import random\n","import gc\n","from itertools import product\n","\n","import pandas as pd\n","import numpy as np\n","import lightgbm as lgb\n","import xgboost as xgb\n","\n","SEED = 42\n","warnings.filterwarnings(\"ignore\")\n","random.seed(SEED)\n","np.random.seed(SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1668128392647,"user":{"displayName":"Divyansh Tripathi","userId":"01798682203976314315"},"user_tz":360},"id":"nU0QGqEuwdex"},"outputs":[],"source":["# Path constants\n","DATA_DIR = \"../../processed_data/\"\n","\n","BASE = DATA_DIR+\"base_sales.pkl\"\n","PRICE = DATA_DIR+\"price_features.pkl\"\n","CALENDAR = DATA_DIR+\"calendar_feats.pkl\"\n","TS_FEATS = DATA_DIR+\"time_based_feats.pkl\"\n","CAT_ENC = DATA_DIR+\"cat_encodings.pkl\"\n","\n","BASE_DIR = \"../../\"\n","MODEL_NAME = \"lgb\"\n","TEST_DATA_DIR = BASE_DIR + f\"models/{MODEL_NAME}/\"\n","MODELS_DIR = BASE_DIR + f\"models/{MODEL_NAME}/\"\n","MODEL_FILE_PREFIX = f\"{MODEL_NAME}_model_\"\n","\n","TARGET_COL = \"units_sold\"\n","PRED_LENGTH = 28\n","\n","TRAIN_END = 1941\n","\n","# Change this list to lists we want to iterate through (unique values in the hierarchical columns)\n","CATEGORIES = ['HOBBIES', 'HOUSEHOLD', 'FOODS']\n","STATES = [\"CA\",\"TX\",\"WI\"]\n","STORES = [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\", \"TX_1\", \"TX_2\", \"TX_3\", \"WI_1\", \"WI_2\", \"WI_3\"]\n","DEPTS = ['HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2', 'FOODS_1' ,'FOODS_2','FOODS_3']\n","\n","HIERARCHY_COL = [\"store_id\",\"dept_id\"]\n","\n","# Include hierarchy cols in the drop features (as they will be constant for each model iteration)\n","# Remove the hierarchy cols not being used. \n","# Example: If you're modelling by category, don't include store/depratment id in the drop features\n","drop_features = [\n","                \"item_id\",\n","                \"dept_id\",\n","                \"cat_id\",\n","                \"store_id\",\n","                \"state_id\",\n","                \"date\",\n","                \"wm_yr_wk\",\n","                \"weekday\"]\n","METRIC = \"rmse\""]},{"cell_type":"markdown","metadata":{},"source":["### Data Preparation\n","We join all the features created from the various datasets and take a subset based on the hierarchical column values. We then split the data where day 1 to 1913 is the training data, 1914 to 1941 is the validation data and 1942 to 1969 is the test data currently with target column empty (eventually populated with the predictions). We save the test file which is used to generate predictions after the model is trained."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":29248,"status":"ok","timestamp":1668128426359,"user":{"displayName":"Divyansh Tripathi","userId":"01798682203976314315"},"user_tz":360},"id":"ogcKtW7lCXNU"},"outputs":[],"source":["def get_data_by_subset(col_names, col_ids):\n","    \"\"\"\n","    Read and concat all the created features for a single value of a category/hierearchy\n","    `col_names` should be a list of all the column names we want to subset with.\n","    `col_ids` should be specific values we want the subset dataframe to have.\n","    \"\"\"\n","    ## Works for multiple column subset\n","    df = pd.read_pickle(BASE)\n","    for index, column in enumerate(col_names):\n","        df = df[(df[column]==col_ids[index])]\n","\n","    price_feats = pd.read_pickle(PRICE).loc[df.index,:].iloc[:,2:]\n","    calendar_feats = pd.read_pickle(CALENDAR).loc[df.index,:].iloc[:,2:]\n","    enc_feats = pd.read_pickle(CAT_ENC).loc[df.index,:].iloc[:,2:]\n","    lag_feats = pd.read_pickle(TS_FEATS).loc[df.index,:].iloc[:,2:]\n","    \n","    # Adding mean and std features \n","    # (but we do not include the mean and std for the hierarchical columns)\n","    remove_col_prefix = col_names[:]+[\"_\".join(col_names)]\n","    excluded_encoding = [col+suffix for col in remove_col_prefix for suffix in [\"_mean\",\"_std\"]]\n","    encoding_features = [i for i in enc_feats.columns if i not in excluded_encoding]\n","    enc_feats = enc_feats.loc[:, encoding_features]\n","    df = pd.concat([df,\n","                    price_feats,\n","                    calendar_feats,\n","                    enc_feats,\n","                    lag_feats],\n","                    axis=1)\n","    df[\"d\"] = df[\"d\"].str.replace(\"d_\",\"\").astype(np.int16)\n","    \n","    del price_feats, calendar_feats, enc_feats, lag_feats\n","    # Subsetting columns with only the required features\n","    features = [column for column in df.columns if column not in drop_features]\n","    df = df[features]\n","    df = df.reset_index(drop=True)\n","    \n","    # Id is needed in the test data but not training\n","    features.remove(TARGET_COL)\n","    features.remove(\"id\")\n","    return df, features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_data(df: pd.DataFrame, col_ids: list):\n","    train_indices = df[\"d\"]<=TRAIN_END\n","    valid_indices = train_indices & (df[\"d\"]>(TRAIN_END-PRED_LENGTH))\n","    preds_indices = (df[\"d\"]>(TRAIN_END-100)) & (df[\"d\"] <= TRAIN_END+PRED_LENGTH)\n","\n","    train_features = df[train_indices][MODEL_FEATURES]\n","    train_labels= df[train_indices][TARGET_COL]\n","\n","    valid_features = df[valid_indices][MODEL_FEATURES]\n","    valid_labels = df[valid_indices][TARGET_COL]\n","\n","    test = df[preds_indices].reset_index(drop=True)\n","    del df\n","\n","    substitute = test[TARGET_COL].values\n","    substitute[(test[\"d\"] > TRAIN_END)] = np.nan\n","    test[TARGET_COL] = substitute\n","\n","    # saving test data\n","    file_uid = \"_\".join(col_ids)\n","    test.to_pickle(MODELS_DIR+\"test_data_\"+file_uid+\".pkl\")\n","    del test, substitute\n","    gc.collect()\n","    return valid_features,valid_labels,train_features,train_labels"]},{"cell_type":"markdown","metadata":{},"source":["### Tuning\n","For both XGBoost and LightGBM we create a hyperparameter space in which we can search through a huge number of configurations. We explore all the major paramters with wide ranges i.e. big search space. These wide ranges for parameters means that we can search through alot of options and configurations giving us a good chance to find the best set of hyperparameters. We use bayesian optimization to save time (as grid search would be very time taking for such huge search space and random search has no guarentee that our solution will be close to optimal).\n","\n","### XGBoost\n","__Normalization__: Since xGBoost is an ensemble model of decision trees we do not need to normalize our data.\n","\n","__Hyperparameter Tuning__: In XGBoost we explore all the major paramters with wide ranges. These wide ranges for parameters means that we can search through alot of options and configurations giving us a good chance to find the best set of hyperparameters.\n","\n","- `n_estimators`: For Number of estimators we search from 500 to 3000 decision trees in the model.\n","- `learning_rate`: We vary our learning rate from 0.01 to 0.1\n","- `subsample`: The subsample (fraction) of the data taken varies from 50% of the data to 100%\n","- `gamma`: From 0 to 0.5\n","- `min_child_weight`: The minimum child weight for each leaf node varies from 1 to 6\n","- `max_depth`: The maximum depth of the tree varies from 5 to 50\n","- `lambda`: The L2 regularization parameter varies from 0.25 to 1\n","- `alpha`: The L1 regularization parameter varies from 0.25 to 1\n","\n","We run 50 different configurations of the model.\n","\n","### LightGBM\n","__Normalization__: Since `lightGBM` is an ensemble model of decision trees we do not need to normalize our data.\n","\n","__Hyperparameter Tuning__: In `lightGBM` we explore the following hyperparamters:\n","- `num_leaves`: We vary the number of leaves in each tree from 5 to 500.\n","- `learning_rate`: The learning rate of the `lightGBM` model varies from 0.001 to 0.2\n","- `max_depth`: We vary the depth of the decision tree from 5 to 50\n","- `n_estimators`: The number of estimator trees varies from 500 to 3000\n","- `min_child_samples`: We vary the minimum child samples required in each tree from 0.001 to 0.2\n","- `reg_lambda`: The L2 regularization parameter varies from 0.1 to 1\n","- `reg_alpha`: The L2 regularization parameter varies from 0.1 to 1\n","- `colsample_bytree`: We vary the number of features taken in a tree from 0.5 (50% of all the features) to 1.\n","\n","### Running Tuning and Final Training\n","We run 50 different configurations for each hierarchical level for both of these model on a smaller sample of the dataset containing around 10% of the total item ids (349). We use a custom class `ModelExecutor` which abstracts all this and generates the best param space corresponding to the lowest RMSE. This hyperparameter configuration is then used to train the final model with the whole dataset. This process is repeated as we iterate through the various hierarchical levels (which can be one or multiple)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tuning constants\n","OBJECTIVE = \"min\"\n","TASK = \"regression\"\n","NUM_TRIALS = 50\n","SAMPLE_SIZE = 349\n","\n","# Light GBM Hyper paramter space\n","lgb_hp_space = {\"num_leaves\": (2, 50, 2),\n","            \"max_depth\": (2, 50, 2),\n","            \"learning_rate\": (0.01, 0.1, 0.01),\n","            \"n_estimators\": (20, 500, 20),\n","            \"min_child_samples\": (5, 100, 5), \n","            \"reg_lambda\": (0.1, 1, 0.1),\n","            \"reg_alpha\": (0.1, 1, 0.1),\n","            \"boosting\": [\"gbdt\",\"dart\"],\n","            \"max_bin\": (50, 200, 50),\n","            \"tweedie_variance_power\": (1.0, 1.5, 0.1),\n","            \"subsample\": (0.4, 0.9, 0.1),\n","            \"feature_fraction\": (0.5, 1, 0.1),\n","            \"subsample_freq\": 1,\n","            \"random_state\": 42,\n","            \"objective\": \"tweedie\",\n","            \"metric\": \"rmse\"}\n","\n","xgb_hp_space = {\n","    \"n_estimators\": (500,3000,500),\n","    \"learning_rate\": (0.01,0.1,0.01),\n","    \"subsample\": (0.5, 1, 0.1),\n","    \"gamma\": (0,0.5,0.1),\n","    \"min_child_weight\": (1,6,2),\n","    \"max_depth\": (5,50,5),\n","    \"lambda\": (0.25,1,0.25),\n","    \"alpha\": (0.25,1,0.25),\n","    \"colsample_bytree\": (0.5, 1, 0.1),\n","    \"objective\": 'reg:squarederror',\n","    \"random_state\": 42,\n","    \"eval_metric\": METRIC,\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TRYING TUNING\n","from basic_ml import ModelExecutor\n","\n","def tune_model(valid_features,\n","                valid_labels,\n","                train_features,\n","                train_labels,\n","                col_ids,\n","                model_class,\n","                model_name,\n","                tune_idx,\n","                model_hp_space: dict={},\n","                model_fit_params: dict={}):\n","    \n","    # Tuning models for the sample ids\n","    model_tuner = ModelExecutor(model_class=model_class,\n","                            x_train=train_features[tune_idx],\n","                            y_train=train_labels[tune_idx],\n","                            param_grid=model_hp_space,\n","                            fit_params=model_fit_params,\n","                            objective=OBJECTIVE,\n","                            normalize=False,\n","                            cv=False,\n","                            task=TASK,\n","                            internal_val=True,\n","                            x_val=valid_features[tune_idx],\n","                            y_val=valid_labels[tune_idx],\n","                            final_train_flag=False)\n","    model_tuner.execute(max_evals=NUM_TRIALS)\n","    # performing final model training on the whole dataset\n","    final_model = model_tuner.base_model.set_params(**model_tuner.best_params)\n","    final_model.fit(pd.concat([train_features,valid_features]),\n","                        pd.concat([train_labels,valid_labels]))\n","    # Saving model and clean up\n","    del valid_features, valid_labels, train_features, train_labels, model_tuner\n","    save_model(col_ids, model_name, final_model)\n","    gc.collect()\n","\n","def save_model(col_ids, model_name, model):\n","    \"\"\"\n","    Save model using the column id and model name.\n","    Directory should be created prior to running the function.\n","    \"\"\"\n","    print(\"model trained for: \", col_ids)\n","    file_uid = \"_\".join(col_ids)\n","    model_path = MODELS_DIR+model_name+\"_\"+file_uid+\".pkl\"\n","    with open(model_path, \"wb\") as f:\n","        pickle.dump(model, f)\n","    del model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Running tuning for all stores & categories\n","for col_ids_iter in list(product(STORES, DEPTS)):\n","    col_ids = list(col_ids_iter) # Converting tuple to list\n","    \n","    sub_df, MODEL_FEATURES = get_data_by_subset(col_names=HIERARCHY_COL,\n","                                    col_ids=col_ids)\n","    random_idx = np.random.randint(0, SAMPLE_SIZE, SAMPLE_SIZE)\n","    tune_idx = sub_df['id'].isin(sub_df['id'].unique()[random_idx])\n","\n","    valid_features,valid_labels,train_features,train_labels = split_data(sub_df, col_ids)\n","    del sub_df\n","\n","    tune_model(valid_features,\n","                valid_labels,\n","                train_features,\n","                train_labels,\n","                col_ids=col_ids,\n","                model_class=lgb.LGBMRegressor,\n","                model_name=f\"{MODEL_NAME}_model\",\n","                model_hp_space=lgb_hp_space,\n","                tune_idx=tune_idx)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMLAfKpLnwr+nvV+0Z4uHVL","collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"5290aa75faf23a16319909e19adc2cd8a2a5bf93f1976f81386266c3c2bf7942"}}},"nbformat":4,"nbformat_minor":0}
