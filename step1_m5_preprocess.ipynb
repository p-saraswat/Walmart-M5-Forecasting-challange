{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5419138",
   "metadata": {},
   "source": [
    "# M5 Forecasting Accuracy\n",
    "#### Estimate the unit sales of Walmart retail goods\n",
    "\n",
    "### Project Description\n",
    "This project presents hierarchical sales data from Walmart, the worldâ€™s largest company by revenue. The aim is to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events. Together, this robust dataset can be used to improve forecasting accuracy.\n",
    "\n",
    "### Problem Statement\n",
    "Walmart wants to know the accurate point forecasts for the upcoming 28 days of sale. \n",
    "\n",
    "#### Point forecasts:\n",
    "The accuracy of the point forecasts will be evaluated using the Root Mean Squared Scaled Error (RMSSE), which is a variant of the well-known Mean Absolute Scaled Error (MASE).\n",
    "\n",
    "\n",
    "### About the Dataset\n",
    "M5 is a Walmart dataset about products sold in the stores of USA. It has a grouped time series format. Data involves unit sales of 3,049 products, classified in 3 product categories (Hobbies, Foods, and Household) and 7 product departments, in which the above-mentioned categories are disaggregated. The products are sold across ten stores, located in three States (CA, TX, and WI). The time range of dataset is from 2011-01-29 to 2016-06-19. The dataset consists of the following 3 tables:\n",
    "1. Calendar: Contains information about the dates the products are sold\n",
    "2. Sell prices: Contains information about the price of the products sold per store and date\n",
    "3. Sales train: Contains the historical daily unit sales data per product and store\n",
    "\n",
    "Please see the below illustration for details:\n",
    "![image.png](levels.png)\n",
    "\n",
    "### Approach Summary\n",
    "We started with a high level data exploration of M5 dataset to understand details and variation. Then we did feature engineering to create features on a product sale level. The features included defining lags, rolling means and SNAP program details. Then we used different Hierarchy of data set for modelling based on stores, departments and categories in the dataset.\n",
    "We tried to use Neural Networks as well as Ensemble Tree-based Regressors at many different levels. The results for the best models (several others were experimented with but were not trained fully as the initial results were subpar)used were as follows:\n",
    "| Model Name | Model Hierarchy Level | Total Number of Models | Model Score |\n",
    "|---|---|---|---|\n",
    "| LightGBM | Per Store | 10 | 0.6229 |\n",
    "| XGBoost | Per Store | 10 | 0.6211 |\n",
    "| GRU | Per Store | 10 | 0.8026 |\n",
    "| Wavenet | Per Store | 10 | 16.7120 |\n",
    "| LSTM | Per Store | 10 | 0.9430 |\n",
    "| LightGBM | Per Store Per Category | 30 | 0.6292 |\n",
    "| LightGBM | Per State Per Category | 30 | 0.64231 |\n",
    "| LightGBM | Per Department | 10 | 2.138 |\n",
    "| __LightGBM__ | __Per Store Per Department__ | __70__ | __0.52978__ |\n",
    "<!-- ![table.png](table.png) -->\n",
    "\n",
    "1. Store-wise level models: RNN, LSTM, GRU, Wavenet, XGB and LGBM\n",
    "2. Department-wise level models: LGBM\n",
    "3. State and category-wise level model: LGBM\n",
    "4. Store and category-wise level model: XGBoost and LGBM\n",
    "5. Store and Department-wise level: LGBM\n",
    "\n",
    "After creating these models and tuning the best hyperparameters for them, we made the forecast prediction of future sales of 28 days with an WRMSSE of __0.52978__.\n",
    "\n",
    "![image](./final_result.jpeg \"Best Kaggle Private Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ff7f6",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The current data is in a wide format i.e. each unique timeseries (identified by id) is a row with all the timesteps (days 1 to 1941) populating seperate columns. Since the scikit-learn regressor models expect a long format we need to transform this data intor the long-format.\n",
    "\n",
    "In our final long-format data each observation would be per id per day, i.e. each unique observation is identified by the sales, prices, events etc on a particular day for that id. \n",
    "\n",
    "### Feature List\n",
    "- `d`: the day of sale (ranging from 1 to 1941)\n",
    "- `sell_price`\n",
    "- `price_norm`: Normalized Price\n",
    "- `discount`: Discount from the actual Price\n",
    "- `month`: The month of the year (encoded as a fourier series), only used in neural networks\n",
    "- `year`: The year, derived from the date.\n",
    "- `snap_CA`: Columns in dicating if SNAP program was applicable on that day for that state or not.\n",
    "- `snap_TX`: Columns in dicating if SNAP program was applicable on that day for that state or not.\n",
    "- `snap_WI`: Columns in dicating if SNAP program was applicable on that day for that state or not.\n",
    "- `Cultural`: Columns indicating different types of cultural events and holidays\n",
    "- `National`: Columns indicating different types of National events and holidays\n",
    "- `Religious`: Columns indicating different types of Religious events and holidays\n",
    "- `Sporting`: Columns indicating different types of Sporting events and holidays\n",
    "- `weekend`: Binary indicator of weekend\n",
    "- `day_of_month`: The month of the year (encoded as a fourier series), only used in neural networks\n",
    "- `state_id_mean`: Encoding for Categorical Variable, mean sales for that particular state\n",
    "- `state_id_std`: Encoding for Categorical Variable, standard deviation of sales for that particular state\n",
    "- `store_id_mean`: Encoding for Categorical Variable, mean sales for that particular Store\n",
    "- `store_id_std`: Encoding for Categorical Variable, standard deviation of sales for that particular Store\n",
    "- `cat_id_mean`: Encoding for Categorical Variable, mean sales for that particular category\n",
    "- `cat_id_std`: Encoding for Categorical Variable, standard deviation of sales for that particular category\n",
    "- `dept_id_mean`: Encoding for Categorical Variable, mean sales for that particular department\n",
    "- `dept_id_std`: Encoding for Categorical Variable, standard deviation of sales for that particular department\n",
    "- `item_id_mean`: Encoding for Categorical Variable, mean sales for that particular item\n",
    "- `item_id_std`: Encoding for Categorical Variable, standard deviation of sales for that particular item\n",
    "- `store_id_item_id_mean`: Encoding for Categorical Variable, mean sales for that particular item in that state\n",
    "- `store_id_item_id_std`: Encoding for Categorical Variable, standard deviation of sales for that particular item in that state\n",
    "- `cat_id_item_id_mean`: Encoding for Categorical Variable, mean sales for that particular item in that category\n",
    "- `cat_id_item_id_std`: Encoding for Categorical Variable, standard deviation of sales for that particular item in that category\n",
    "- `dept_id_item_id_mean`: Encoding for Categorical Variable, mean sales for that particular item in that department\n",
    "- `dept_id_item_id_std`: Encoding for Categorical Variable, standard deviation of sales for that particular item in that department\n",
    "- `lag28`: Lag sales of 28 days from the current row.\n",
    "- `lag29`: Lag sales of 29 days from the current row.\n",
    "- `lag30`: Lag sales of 30 days from the current row.\n",
    "- `lag31`: Lag sales of 31 days from the current row.\n",
    "- `lag32`: Lag sales of 32 days from the current row.\n",
    "- `lag33`: Lag sales of 33 days from the current row.\n",
    "- `lag34`: Lag sales of 34 days from the current row.\n",
    "- `lag35`: Lag sales of 35 days from the current row.\n",
    "- `lag36`: Lag sales of 36 days from the current row.\n",
    "- `lag37`: Lag sales of 37 days from the current row.\n",
    "- `lag38`: Lag sales of 38 days from the current row.\n",
    "- `lag39`: Lag sales of 39 days from the current row.\n",
    "- `lag40`: Lag sales of 40 days from the current row.\n",
    "- `lag41`: Lag sales of 41 days from the current row.\n",
    "- `rolling_mean7`: Rolling mean of 7 days prior to current row. (we also have to take a lag of 28 days otherwise this may be null when we calculate the prediction for the 28th day of forecast)\n",
    "- `rolling_std7`: Rolling standard deviation of d7 days prior to current row.\n",
    "- `rolling_mean14`: Rolling mean of 14 days prior to current row.\n",
    "- `rolling_std14`: Rolling standard deviation of 14 days prior to current row.\n",
    "- `rolling_mean30`: Rolling mean of 30 days prior to current row.\n",
    "- `rolling_std30`: Rolling standard deviation of 30 days prior to current row.\n",
    "- `rolling_mean60`: Rolling mean of 60 days prior to current row.\n",
    "- `rolling_std60`: Rolling standard deviation of 60 days prior to current row.\n",
    "- `rolling_mean90`: Rolling mean of 90 days prior to current row.\n",
    "- `rolling_std90`: Rolling standard deviation of 90 days prior to current row.\n",
    "- `rolling_mean180`: Rolling mean of 180 days prior to current row.\n",
    "- `rolling_std180`: Rolling standard deviation of 180 days prior to current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d70b076a",
   "metadata": {
    "id": "d70b076a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "742bb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_LENGTH = 28\n",
    "TRAINING_END = 1941\n",
    "TARGET_COL = 'units_sold'\n",
    "CAT_COLS = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "LAGS = [i for i in range(PRED_LENGTH, PRED_LENGTH+14)]\n",
    "ROLL_RANGE = [7,14,30,60,90,180]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4009a0",
   "metadata": {},
   "source": [
    "### Adding features from Calender\n",
    "1. Since the Null values in the calendar indicate that no event occured we fill the this as the dfeault value in the `event_type_1` and `event_type_2` columns.\n",
    "2. We create dummy values for all the different types of the event types, thereby getting seperate columns for each event type: `Cultural`, `National`, `Religious`, `Sporting`\n",
    "3. We also use the snap codes for the different states as they indicate if the snap food stamps were applicable for that particular day in that state (TX, CA or WI).\n",
    "4. Finally we create some features based on dates themselves: such as a `weekend` flag, the month, the `day_of_month` and the year so as to assist our models in identifying the temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba6a294",
   "metadata": {
    "id": "2ba6a294"
   },
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('data/calendar.csv')\n",
    "calendar[['event_type_1','event_type_2']].fillna(\"no event\", inplace=True)\n",
    "calendar = pd.get_dummies(calendar, columns=['event_type_1','event_type_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cb4efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar[\"event_type_1_Cultural\"] = calendar[\"event_type_1_Cultural\"]+calendar[\"event_type_2_Cultural\"]\n",
    "calendar[\"event_type_1_Religious\"] = calendar[\"event_type_1_Religious\"]+calendar[\"event_type_2_Religious\"]\n",
    "calendar.drop(columns=[\"event_type_2_Cultural\",\n",
    "                        \"event_type_2_Religious\",\n",
    "                        \"event_name_1\",\n",
    "                        \"event_name_2\",\n",
    "                        \"wday\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562a3d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.columns = [col.replace(\"event_type_1_\",\"\") for col in calendar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "110e5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar[\"date\"] = pd.to_datetime(calendar[\"date\"])\n",
    "calendar[\"weekend\"] = (calendar[\"date\"].dt.day_of_week>4).astype(np.int8)\n",
    "calendar[\"day_of_month\"] = calendar[\"date\"].dt.day\n",
    "calendar.drop(columns=[\"date\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55dae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train = pd.read_csv('data/sales_train_evaluation.csv')\n",
    "# Creating dummy columns for future values as well\n",
    "for i in range(PRED_LENGTH):\n",
    "    sales_train[\"d_\"+str(TRAINING_END+i+1)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7784d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train = pd.melt(sales_train, \n",
    "       id_vars=['id']+CAT_COLS,\n",
    "       var_name='d',\n",
    "       value_name = TARGET_COL)\n",
    "# we only keep the training rows where the sales are non null\n",
    "sales_train = sales_train[(sales_train[TARGET_COL].notnull()) | (sales_train['d']>\"d_\"+str(TRAINING_END))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6fc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving calendar and time-based features\n",
    "calendar = sales_train[[\"id\",\"d\"]].merge(calendar, on=\"d\")\n",
    "calendar.to_pickle('processed_data/calendar_feats.pkl')\n",
    "del calendar\n",
    "sales_train.to_pickle('processed_data/base_sales.pkl')\n",
    "del sales_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcc818a",
   "metadata": {},
   "source": [
    "### Adding Features derived from Sales\n",
    "1. __Time-based__: \n",
    "    - Lags: The events in the recent past are often a good predictor of the future events, we try to include this in our data using a 2 weeks lag period. We choose 2-weeks as the general population often gets their paychecks biweekly, and thus we can expect some seasonality where events ocurring `t-2 weeks` ago would occur again at time `t`.\n",
    "    - Rolling means and standard deviations: Another way to capture seasonality is to use the rolling means of the historical data prior to an event. Many statistical time series models such as ARIMA use these rolling means to determine the future predictions (using past moving average as the trend). We also include the standard deviation as it is an indicator of the volatility of a timeseries i.e. how much y varied in the window we choose.\n",
    "        - We choose many different windows to calculate the rolling statistics: 7,14,30,60,90,180. The intent is to capture short term trends such as weekly/biweekly along with long-term trends such as quarterly or semi annual.\n",
    "2. __Hierarchy Based__: Given that there are a huge number of categorical levels when we consider the `store`, `item`, `category` and `department` variables, creating dummy columns for each of them would make our training data sparse (very high dimensioanlity may be problematic). Thus we have tried to do an encoding based on the behavior of the prices and sales in these hierarchies. we consider the means and standard deviations of the sales in a particular category, department, store or state. Additionally, we have also created features with nested hierarchies with `item_id`. We try to add features that capture the sales behviour of an item in a particular category, department, store or state.\n",
    "    - Store/Category/Depart means and std\n",
    "    - Nested item-wise means and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcdd1209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag variables\n",
      "Creating rolling variables\n"
     ]
    }
   ],
   "source": [
    "# Reading base data and only using relevant columns\n",
    "sales_train = pd.read_pickle('processed_data/base_sales.pkl')\n",
    "sales_train = sales_train[[\"id\",\"d\",TARGET_COL]]\n",
    "print(\"Creating lag features\")\n",
    "\n",
    "for i in LAGS:\n",
    "    sales_train[\"lag\"+str(i)] = np.nan\n",
    "    for item, item_df in sales_train[[\"id\",TARGET_COL]].groupby(\"id\"):\n",
    "        sales_train.loc[item_df.index,\"lag\"+str(i)] = item_df[TARGET_COL].shift(i).astype(np.float16)\n",
    "\n",
    "print(\"Creating rolling features\")\n",
    "for i in ROLL_RANGE:\n",
    "    sales_train[\"rolling_mean\"+str(i)] = np.nan\n",
    "    sales_train[\"rolling_std\"+str(i)] = np.nan\n",
    "    for item, item_df in sales_train[[\"id\",TARGET_COL]].groupby(\"id\"):\n",
    "        sales_train.loc[item_df.index,\"rolling_mean\"+str(i)] = item_df[TARGET_COL].shift(PRED_LENGTH).rolling(i).mean().astype(np.float16)\n",
    "        sales_train.loc[item_df.index,\"rolling_std\"+str(i)] = item_df[TARGET_COL].shift(PRED_LENGTH).rolling(i).std().astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving lag variables\n",
    "sales_train.drop(columns=[TARGET_COL]).to_pickle('processed_data/time_based_feats.pkl')\n",
    "del sales_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44cb76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading base data and only using relevant columns\n",
    "sales_train = pd.read_pickle('processed_data/base_sales.pkl')\n",
    "sales_train = sales_train[['id', 'd',TARGET_COL]+CAT_COLS]\n",
    "\n",
    "# Adding encoded means and standard deviations for the various hierarchies\n",
    "groups =  [['state_id'],['store_id'],['cat_id'],['dept_id'],['item_id'],\n",
    "            ['store_id', 'item_id'],\n",
    "            ['cat_id', 'item_id'],\n",
    "            ['dept_id', 'item_id']]\n",
    "\n",
    "### Adding Encodings\n",
    "for group in groups:\n",
    "    col_name = '_'.join(group)+'_'\n",
    "    cols = group+[TARGET_COL]\n",
    "    sales_train[col_name+'mean'] = sales_train.groupby(group)[TARGET_COL].transform('mean').astype(np.float16)\n",
    "    sales_train[col_name+'std'] = sales_train.groupby(group)[TARGET_COL].transform('std').astype(np.float16)\n",
    "\n",
    "# Saving encoding features\n",
    "sales_train.drop(columns=CAT_COLS+[TARGET_COL]).to_pickle('processed_data/cat_encodings.pkl')\n",
    "del sales_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd71c325",
   "metadata": {},
   "source": [
    "### Adding Price Features\n",
    "Price is perhaps the biggest influence on the demand of a particular product. In order to aid our model to know the variation in price we create two additional features:\n",
    "- Normalized price\n",
    "- Discount from mean: It is an indicator of how much above/below the overall mean for this product is the current price? It can help us in better predicting increment in sales when there is a markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26f3a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading base data and only using relevant columns\n",
    "# sales_train = pd.read_pickle('processed_data/base_sales.pkl')\n",
    "sales_train = sales_train[['id', 'd',TARGET_COL]+CAT_COLS]\n",
    "# Reading and merging calender feats to use as a week to day map\n",
    "week_map = pd.read_pickle(\"processed_data/calendar_feats.pkl\")[[\"wm_yr_wk\"]]\n",
    "sales_train = pd.concat([sales_train,week_map],axis=1)\n",
    "del week_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e216ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the prices\n",
    "sell_prices = pd.read_csv('data/sell_prices.csv')\n",
    "# creating id column for join\n",
    "sell_prices[\"id\"] = sell_prices[\"item_id\"]+\"_\"+sell_prices[\"store_id\"]+\"_evaluation\"\n",
    "sell_prices.drop(columns=[\"store_id\",\"item_id\"], inplace=True)\n",
    "sell_prices.sort_values(by=[\"id\",\"wm_yr_wk\"], inplace=True)\n",
    "# merging with sales dataset\n",
    "sales_train = sales_train.merge(sell_prices, how=\"left\", on=[\"id\",\"wm_yr_wk\"])\n",
    "del sell_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24679cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train['price_norm'] = np.nan\n",
    "sales_train['discount'] = np.nan\n",
    "for item, df in sales_train[[\"id\",\"sell_price\"]].groupby(\"id\"):\n",
    "    sales_train.loc[df.index, \"price_norm\"] = StandardScaler().fit_transform(df[\"sell_price\"].values.reshape(-1, 1))\n",
    "    price_mean = df[\"sell_price\"].mean()\n",
    "    sales_train.loc[df.index, \"discount\"] = (df[\"sell_price\"]-price_mean)/price_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c9d019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the price-based features\n",
    "sales_train = sales_train[[\"id\",\"d\",\"sell_price\",\"price_norm\",\"discount\"]]\n",
    "sales_train.to_pickle('processed_data/price_features.pkl')\n",
    "del sales_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c591b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5290aa75faf23a16319909e19adc2cd8a2a5bf93f1976f81386266c3c2bf7942"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
